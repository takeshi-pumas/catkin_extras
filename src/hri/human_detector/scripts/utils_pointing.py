#! /usr/bin/env python3
import tf
import cv2
import rospy  
import tf2_ros                                    
from human_detector.srv import Point_detector ,Point_detectorResponse 
from human_detector.srv import Human_detector ,Human_detectorResponse 

from cv_bridge import CvBridge
from object_classification.srv import *
import tf2_ros    
from segmentation.msg import *
import numpy as np
import ros_numpy
import os
from glob import glob
import matplotlib.pyplot as plt
import cv2 
from collections import Counter
from sensor_msgs.msg import Image , LaserScan , PointCloud2
from geometry_msgs.msg import TransformStamped, Pose
from tf2_sensor_msgs.tf2_sensor_msgs import do_transform_cloud
import rospkg

#from utils.misc_utils import TF_MANAGER

#-----------------------------------------------------------------
global tf_listener, ptcld_lis, broadcaster , bridge , net , b_tf, b_st


rospy.init_node('human_pointing_detector') 
tfBuffer = tf2_ros.Buffer()
listener = tf2_ros.TransformListener(tfBuffer)
b_tf=tf2_ros.TransformBroadcaster()
b_st=tf2_ros.StaticTransformBroadcaster()
_tfbuff = tf2_ros.Buffer()

usr_url=os.path.expanduser( '~' )
protoFile = usr_url+"/openpose/models/pose/body_25/pose_deploy.prototxt"
weightsFile = usr_url+"/openpose/models/pose/body_25/pose_iter_584000.caffemodel"
net = cv2.dnn.readNetFromCaffe(protoFile, weightsFile)
#tf_listener = tf.TransformListener()
#broadcaster= tf.TransformBroadcaster()
#tf_static_broadcaster= tf2_ros.StaticTransformBroadcaster()
#pub = rospy.Publisher('/segmented_images', Image, queue_size=1)
bridge=CvBridge()

#-----------------------------------------------------------------
def write_tf(pose, q, child_frame , parent_frame='map'):
    t= TransformStamped()
    t.header.stamp = rospy.Time.now()
    #t.header.stamp = rospy.Time(0)
    t.header.frame_id =parent_frame
    t.child_frame_id =  child_frame
    t.transform.translation.x = pose[0]
    t.transform.translation.y = pose[1]
    t.transform.translation.z = pose[2]
    #q = tf.transformations.quaternion_from_euler(eu[0], eu[1], eu[2])
    t.transform.rotation.x = q[0]
    t.transform.rotation.y = q[1]
    t.transform.rotation.z = q[2]
    t.transform.rotation.w = q[3]
    return t

#-----------------------------------------------------------------
def read_tf(t):
    pose=np.asarray((
        t.transform.translation.x,
        t.transform.translation.y,
        t.transform.translation.z
        ))
    quat=np.asarray((
        t.transform.rotation.x,
        t.transform.rotation.y,
        t.transform.rotation.z,
        t.transform.rotation.w
        ))
    
    return pose, quat

#-----------------------------------------------------------------
def getTF(target_frame='', ref_frame='map'):
        try:
            tf = tfBuffer.lookup_transform(
                ref_frame, target_frame, rospy.Time(0), rospy.Duration(1.5))
            return tf2_obj_2_arr(tf)
        except:
            return [False, False]

#-----------------------------------------------------------------
def tf2_obj_2_arr(transf):
        pos = []
        pos.append(transf.transform.translation.x)
        pos.append(transf.transform.translation.y)
        pos.append(transf.transform.translation.z)

        rot = []
        rot.append(transf.transform.rotation.x)
        rot.append(transf.transform.rotation.y)
        rot.append(transf.transform.rotation.z)
        rot.append(transf.transform.rotation.w)

        return [pos, rot]

#-----------------------------------------------------------------
def change_ref_frame_tf(point_name='', rotational=[0, 0, 0, 1], new_frame='map'):
        try:
            traf = tfBuffer.lookup_transform(
                new_frame, point_name, rospy.Time(0))
            translation, _ = tf2_obj_2_arr(traf)
            print("TRANSLATION",translation)
            t=write_tf(pose=translation,q=rotational,child_frame=point_name,parent_frame=new_frame)
            b_st.sendTransform(t)
            return True
        except:
            return False

#-----------------------------------------------------------------
def probmap_to_3d_mean(points_data,probMap, thres_prob=0.3):
    ##Prob map to 3d point

    mask=np.where(probMap>thres_prob)
    npmask=np.asarray(mask).T

    npmask.shape
    xyz=[]
    if len (npmask)>1:
        for a in npmask:
            ix,iy=a[0],a[1]
            aux=(np.asarray((points_data['x'][ix,iy],points_data['y'][ix,iy],points_data['z'][ix,iy])))
            #print (aux)
            if np.isnan(aux[0]) or np.isnan(aux[1]) or np.isnan(aux[2]):
                    'reject point'
            else:
                xyz.append(aux)

    xyz=np.asarray(xyz)
    if (len(xyz)!=0):
        cent=xyz.mean(axis=0)
    else:
        cent=np.zeros(3)
    return cent

#-----------------------------------------------------------------
def detect_human(points_msg,dist = 6, remove_bkg= True):

    points_data = ros_numpy.numpify(points_msg) 

    if remove_bkg:
        _,frame = removeBackground(points_msg, dist)
    else:
        image_data = points_data['rgb'].view((np.uint8, 4))[..., [2, 1, 0]]   
        frame=cv2.cvtColor(image_data, cv2.COLOR_BGR2RGB)
   
    inHeight = frame.shape[0]
    inWidth = frame.shape[1]

    # Prepare the frame to be fed to the network
    inpBlob = cv2.dnn.blobFromImage(frame, 1.0 / 255, (inWidth, inHeight), (0, 0, 0), swapRB=False, crop=False)

    # Set the prepared object as the input blob of the network
    net.setInput(inpBlob)

    output = net.forward()
    i = 0 #Face
    #i = 1# Neck
    probMap = output[0, i, :, :]
    probMap = cv2.resize(probMap, (inWidth, inHeight))
    cent= probmap_to_3d_mean(points_data,probMap)
    print (cent)
    if np.isnan(cent.any()):return Human_detectorResponse()
    print (cent)
    if np.isnan(cent.any()):cent=np.zeros(3)
    res=Human_detectorResponse()
    res.x= cent[0]
    res.y= cent[1]
    res.z= cent[2]

    return res    

#-----------------------------------------------------------------
def detect_pointing(points_msg,dist = 6, remove_bkg= True):
    """points_data = ros_numpy.numpify(points_msg)    
    image_data = points_data['rgb'].view((np.uint8, 4))[..., [2, 1, 0]]   
    image=cv2.cvtColor(image_data, cv2.COLOR_BGR2RGB)
    
    print (image.shape)
    """
    pts= ros_numpy.numpify(points_msg)  
    #---
    #human, _ =getTF(target_frame='human',ref_frame='head_rgbd_sensor_rgb_frame') 
    
    #print(human)
    #distToTF = np.linalg.norm(human) if human[0] else 2

    #print("DISTANCIA AL HUMANO ",distToTF)

    # <<<<<<<<<<<<<<<<<<<>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
    # HARDCODEADA LA DISTANCIA A 2 METROS (+ 0.3 m) 
    # NECESARIO MODIFICAR PARA PRUEBAS EN ROBOCUP DE ACUERDO A ESPACIOS Y 
    # DISTANCIAS DE LA PRUEBA 
    # SE IBA A UTILIZAR -> distToTF DE ACUERDO A LA TF DE LA PERSONA DETECTADA
    # PERO NO ES MUY EXACTO
    # <<<<<<<<<<<<<<<<<<<>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
    if remove_bkg:
        image, frame = removeBackground(points_msg, dist)
        save_image(frame,name="maskedImage")
    else:
        image_data = pts['rgb'].view((np.uint8, 4))[..., [2, 1, 0]]   
        frame=cv2.cvtColor(image_data, cv2.COLOR_BGR2RGB)
        save_image(frame,name="noMaskedImage")

    #---
    
    inHeight = frame.shape[0]
    inWidth = frame.shape[1]
    # Prepare the frame to be fed to the network
    inpBlob = cv2.dnn.blobFromImage(frame, 1.0 / 255, (inWidth, inHeight), (0, 0, 0), swapRB=False, crop=False)
    # Set the prepared object as the input blob of the network
    net.setInput(inpBlob)
    output = net.forward()
    thresh= 0.45
    poses=[]
    deb_imgr=frame[:,:,0]
    deb_imgg=frame[:,:,1]
    deb_imgb=frame[:,:,2]
    res=Point_detectorResponse()
    for i in np.asarray((3,4,6,7)):
        probMap = output[0, i, :, :]
        probMap = cv2.resize(probMap, (inWidth, inHeight))
        if len(np.where(probMap>=thresh)[0]) ==0: pose=[0,0,0]  
        else:pose=  [np.nanmean(pts['x'][np.where(probMap>=thresh)]),
                     np.nanmean(pts['y'][np.where(probMap>=thresh)]),
                     np.nanmean(pts['z'][np.where(probMap>=thresh)])]
        poses.append(pose)
        #deb_img= probMap+ deb_img*0.3  #DEBUG IMAGE
        deb_imgr[np.where(probMap>=thresh)]= 255
        deb_imgg[np.where(probMap>=thresh)]= 255
        deb_imgb[np.where(probMap>=thresh)]= 255
    deb_img_rgb=cv2.merge((deb_imgr, deb_imgg, deb_imgb))
    res.debug_image.append(bridge.cv2_to_imgmsg(deb_img_rgb))
    save_image(deb_img_rgb,name="debugImgDetectPointing_1")
    #right elbow       ####
    # right wrist      ####
    # left elbow
    # left wrist
    ##############################
    try:
            tt = tfBuffer.lookup_transform('map', 'head_rgbd_sensor_link', rospy.Time())
                        
            trans,rot=read_tf(tt)
            #print ("############head",trans,rot)
    except (tf2_ros.LookupException, tf2_ros.ConnectivityException, tf2_ros.ExtrapolationException):
            print ( 'No head TF FOUND')

    found_joints={}
    for i, name in enumerate(['right_elbow','right_wrist','left_elbow','left_wrist']):
        if np.sum(np.asarray(poses[i]))==0:print(f'no {name} points')
        else:           
            print (poses[i],f' {name} pose wrt head')
            found_joints[name]=poses[i]
            
    print("FOUND JOINTS: ",found_joints)
    vd=[0,0,0]
    if 'right_wrist' in found_joints.keys() and 'right_elbow' in found_joints.keys():
        
        pc_np_array = np.array([(found_joints['right_elbow'][0], found_joints['right_elbow'][1], found_joints['right_elbow'][2]),
                                 (found_joints['right_wrist'][0],found_joints['right_wrist'][1],found_joints['right_wrist'][2])]
         , dtype=[('x', 'f4'), ('y', 'f4'), ('z', 'f4')])
        #pc_np_array = np.array([found_joints['right_elbow'], found_joints['right_wrist'], (0.0, 0.0, 0.0)], dtype=[('x', 'f4'), ('y', 'f4'), ('z', 'f4')])
        points_msg=ros_numpy.msgify(PointCloud2,pc_np_array,rospy.Time.now(),'head_rgbd_sensor_rgb_frame')
        cloud_out = do_transform_cloud(points_msg, tt)
        np_corrected=ros_numpy.numpify(cloud_out)
        corrected=np_corrected.reshape(pc_np_array.shape)
        print(corrected,'########################### Left', found_joints)
        #t=write_tf((corrected['x'][0],corrected['y'][0],corrected['z'][0]),(0,0,0,1),'right_elbow')
        #print (t)
        #b_st.sendTransform(t)
        #t=write_tf((corrected['x'][1],corrected['y'][1],corrected['z'][1]),(0,0,0,1),'right_wrist')
        #print (t)
        #b_st.sendTransform(t)
        elbow_xyz,wrist_xyz=[corrected['x'][0],corrected['y'][0],corrected['z'][0]],[corrected['x'][1],corrected['y'][1],corrected['z'][1]]
        v= np.asarray(wrist_xyz)-np.asarray(elbow_xyz)
        #print("ELBOW RIGHT",elbow_xyz)
        #print("WRIST RIGHT",wrist_xyz)
        vd = [-(wrist_xyz[0] - elbow_xyz[0]), -(wrist_xyz[1]-elbow_xyz[1]),-1-(wrist_xyz[2]-elbow_xyz[2])]
        
        vectD = [wrist_xyz[0]-elbow_xyz[0],wrist_xyz[1]-elbow_xyz[1],wrist_xyz[2]-elbow_xyz[2]]
        alfa = -wrist_xyz[2]/vectD[2]
        y=wrist_xyz[1]+alfa*vectD[1]
        x=wrist_xyz[0]+alfa*vectD[0]
        
        #t= elbow_xyz[2]-   v[2]
        #x= elbow_xyz[0]+ t*v[0]
        #y= elbow_xyz[1]+ t*v[1]
        print(x,y,'x,y')
        t=write_tf((x,y,0),(0,0,0,1),'pointing_right')
        b_st.sendTransform(t)
        res.x_r=x
        res.y_r=y
        res.z_r=0
        #
    else:
        res.x_r=0.0
        res.y_r=0.0
        res.z_r=0.0
        t=write_tf((0,0,0),(0,0,0,1),'pointing_right')
        b_st.sendTransform(t)

    vi=[0,0,0]
    if 'left_wrist'  in found_joints.keys() and 'left_elbow'  in found_joints.keys():
        pc_np_array = np.array([(found_joints['left_elbow'][0], found_joints['left_elbow'][1], found_joints['left_elbow'][2]),(found_joints['left_wrist'][0],found_joints['left_wrist'][1],found_joints['left_wrist'][2])]
         , dtype=[('x', 'f4'), ('y', 'f4'), ('z', 'f4')])
        #pc_np_array = np.array([found_joints['right_elbow'], found_joints['right_wrist'], (0.0, 0.0, 0.0)], dtype=[('x', 'f4'), ('y', 'f4'), ('z', 'f4')])
        points_msg=ros_numpy.msgify(PointCloud2,pc_np_array,rospy.Time.now(),'head_rgbd_sensor_rgb_frame')
        cloud_out = do_transform_cloud(points_msg, tt)
        np_corrected=ros_numpy.numpify(cloud_out)
        corrected=np_corrected.reshape(pc_np_array.shape)
        print(corrected,'###########################Right', found_joints)
        #t=write_tf((corrected['x'][0],corrected['y'][0],corrected['z'][0]),(0,0,0,1),'right_elbow')
        #print (t)
        #b_st.sendTransform(t)
        #t=write_tf((corrected['x'][1],corrected['y'][1],corrected['z'][1]),(0,0,0,1),'right_wrist')
        #print (t)
        #b_st.sendTransform(t)
        elbow_xyz,wrist_xyz=[corrected['x'][0],corrected['y'][0],corrected['z'][0]],[corrected['x'][1],corrected['y'][1],corrected['z'][1]]
        #print("ELBOW LEFT",elbow_xyz)
        #print("WRIST LEFT",wrist_xyz)
        v= np.asarray(wrist_xyz)-np.asarray(elbow_xyz)
        vi = [-(wrist_xyz[0] - elbow_xyz[0]), -(wrist_xyz[1]-elbow_xyz[1]),-1-(wrist_xyz[2]-elbow_xyz[2])]
        vectD = [wrist_xyz[0]-elbow_xyz[0],wrist_xyz[1]-elbow_xyz[1],wrist_xyz[2]-elbow_xyz[2]]
        alfa = -wrist_xyz[2]/vectD[2]
        y=wrist_xyz[1]+alfa*vectD[1]
        x=wrist_xyz[0]+alfa*vectD[0]
        
        #t= elbow_xyz[2]-   v[2]
        #x= elbow_xyz[0]+ t*v[0]
        #y= elbow_xyz[1]+ t*v[1]
        print(x,y, v,'x,y')
        t=write_tf((x,y,0),(0,0,0,1),'pointing_left')
        b_st.sendTransform(t)
        res.x_l=x
        res.y_l=y
        res.z_l=0
        
    else:

        res.x_l=0.0
        res.y_l=0.0
        res.z_l=0.0
        t=write_tf((0,0,0),(0,0,0,1),'pointing_left')
        b_st.sendTransform(t)

    
    if np.linalg.norm(vd)>np.linalg.norm(vi):
        print("Mano DERECHA levantada")
        res.x_l = -1.0
        res.y_l = -1.0
        res.z_l = -1.0

    else:
        print("Mano IZQUIERDA levantada")
        res.x_r = -1.0
        res.y_r = -1.0
        res.z_r = -1.0
        
    
    return res

#-----------------------------------------------------------------
def detect_pointing2(points_msg,dist = 6,remove_bkg= True):
    #tf_man = TF_MANAGER()
    res=Point_detectorResponse()
    points_data = ros_numpy.numpify(points_msg)
    if remove_bkg:
        image, masked_image = removeBackground(points_msg,distance = dist)
        save_image(masked_image,name="maskedImage")
    else:
        
        image_data = points_data['rgb'].view((np.uint8, 4))[..., [2, 1, 0]]   
        frame=cv2.cvtColor(image_data, cv2.COLOR_BGR2RGB)
        save_image(frame,name="noMaskedImage")
    #image_data = points_data['rgb'].view((np.uint8, 4))[..., [2, 1, 0]]   
    #image=cv2.cvtColor(image_data, cv2.COLOR_BGR2RGB)
    #pts= points_data
    
    inHeight = image.shape[0]
    inWidth = image.shape[1]
    # Prepare the frame to be fed to the network
    inpBlob = cv2.dnn.blobFromImage(masked_image, 1.0 / 255, (inWidth, inHeight), (0, 0, 0), swapRB=False, crop=False)
    # Set the prepared object as the input blob of the network
    net.setInput(inpBlob)
    output = net.forward()
    try:
        # Logica para separar esqueletos en una imagen
        poses = getconectionJoints(output,inHeight,inWidth)
        imageDraw = drawSkeletons(image,poses,plot=False)
        save_image(imageDraw,name="maskedImageWithOPinOpenCV")
    
    except Exception as e:
        print("Ocurrio un error al construir el esqueleto",e,type(e).__name__)
        raise Exception("Ocurrio un error al construir el esqueleto ")

    res.debug_image.append(bridge.cv2_to_imgmsg(imageDraw))
    # HASTA AQUI NO REQUIERE NUBE DE PUNTOS

    dists=[]
    for i,pose in enumerate(poses):
        if pose[0,0] != 0:
            print(pose[0,0],pose[0,1],pose[0])
            pose_xyz =[points_data['x'][int(pose[0,1]), int(pose[0,0])],
                       points_data['y'][int(pose[0,1]), int(pose[0,0])],
                       points_data['z'][int(pose[0,1]), int(pose[0,0])]]
            if (pose_xyz == None).any():               # PENDIENTE DE TERMINAR Y PROBAR
                raise Exception("Error al obtener datos de PointCloud")
            dists.append(np.linalg.norm(pose_xyz)) 
            t=write_tf((pose_xyz[0],pose_xyz[1],pose_xyz[2]),(0,0,0,1),'person_'+str(i),parent_frame='head_rgbd_sensor_rgb_frame')
            b_st.sendTransform(t)
            rospy.sleep(0.3)
        elif pose[0,0] == 0 and pose[1,0] != 0:
            print(pose[1,0],pose[1,1])
            pose_xyz =[points_data['x'][int(pose[1,1]), int(pose[1,0])],
                       points_data['y'][int(pose[1,1]), int(pose[1,0])],
                       points_data['z'][int(pose[1,1]), int(pose[1,0])]]
            if (pose_xyz == None).any():               # PENDIENTE DE TERMINAR Y PROBAR
                raise Exception("Error al obtener datos de PointCloud")
            dists.append(np.linalg.norm(pose_xyz))  
            t=write_tf((pose_xyz[0],pose_xyz[1],pose_xyz[2]),(0,0,0,1),'person_'+str(i),parent_frame='head_rgbd_sensor_rgb_frame')
            b_st.sendTransform(t)
            rospy.sleep(0.3)
        else:
            print("NO HAY DATOS PARA PUBLICAR")   
            # PENDIENTE DE TERMINAR Y PROBAR
            #raise Exception("Error, datos en zero para TF")


    print(np.min(dists),np.argmin(dists))
    
    # DE TODAS LAS DISTANCIAS OBTENGO EL INDICE DE LA MAS PEQUEÑA
    k = np.argmin(dists) if len(dists)>1 else 0

    # PUBLICO CODOS Y MANOS DE LA PERSONA k Y OBTENGO COORDENADAS RESPECTO A MAPA    
    codoD =[points_data['x'][int(poses[k,3,1]), int(poses[k,3,0])],
            points_data['y'][int(poses[k,3,1]), int(poses[k,3,0])],
            points_data['z'][int(poses[k,3,1]), int(poses[k,3,0])]]
    codoI =[points_data['x'][int(poses[k,6,1]), int(poses[k,6,0])],
            points_data['y'][int(poses[k,6,1]), int(poses[k,6,0])],
            points_data['z'][int(poses[k,6,1]), int(poses[k,6,0])]]
    manoD =[points_data['x'][int(poses[k,4,1]), int(poses[k,4,0])],
            points_data['y'][int(poses[k,4,1]), int(poses[k,4,0])],
            points_data['z'][int(poses[k,4,1]), int(poses[k,4,0])]]
    manoI =[points_data['x'][int(poses[k,7,1]), int(poses[k,7,0])],
            points_data['y'][int(poses[k,7,1]), int(poses[k,7,0])],
            points_data['z'][int(poses[k,7,1]), int(poses[k,7,0])]]
            
    if (codoD == None).any() or (manoD == None).any() or (codoI == None).any() or (manoI == None).any():   # PENDIENTE DE TERMINAR Y PROBAR
        raise Exception("Error al publicar TF (empty)")
    
    t=write_tf((codoD[0],codoD[1],codoD[2]),(0,0,0,1),'codoD',parent_frame='head_rgbd_sensor_rgb_frame')
    b_st.sendTransform(t)
    rospy.sleep(0.3)
    t=write_tf((manoD[0],manoD[1],manoD[2]),(0,0,0,1),'manoD',parent_frame='head_rgbd_sensor_rgb_frame')
    b_st.sendTransform(t)
    rospy.sleep(0.3)
    #if (codoI == None).any() or (manoI == None).any():               # PENDIENTE DE TERMINAR Y PROBAR
    #    raise Exception("Error al publicar TF (empty)")
    t=write_tf((codoI[0],codoI[1],codoI[2]),(0,0,0,1),'codoI',parent_frame='head_rgbd_sensor_rgb_frame')
    b_st.sendTransform(t)
    rospy.sleep(0.3)
    t=write_tf((manoI[0],manoI[1],manoI[2]),(0,0,0,1),'manoI',parent_frame='head_rgbd_sensor_rgb_frame')
    b_st.sendTransform(t)
    rospy.sleep(0.7)

    change_ref_frame_tf(point_name='codoD')
    change_ref_frame_tf(point_name='codoI')
    change_ref_frame_tf(point_name='manoD')
    change_ref_frame_tf(point_name='manoI')
    rospy.sleep(0.7)

    codoD, _ =getTF(target_frame='codoD')
    codoI, _ =getTF(target_frame='codoI')
    manoD, _ =getTF(target_frame='manoD')
    manoI, _ =getTF(target_frame='manoI')
    rospy.sleep(0.7)

    if (codoD == None).any() or (manoD == None).any() or (codoI == None).any() or (manoI == None).any():   # PENDIENTE DE TERMINAR Y PROBAR
        raise Exception("Error al obtener TF (empty)")
    
    ds=[manoD[2]-codoD[2],manoI[2]-codoI[2]]

    # RESTA DE VECTOR MANO-CODO CON UN VECTOR HACIA ABAJO, DEPENDERA DE LAS COORDENADAS DEL MAPA Y CUAL ES EL VECTOR ARRIBA-ABAJO
    # (0,0,min(izq|der[2])) - (mano-codo)
    vd=[-(manoD[0]-codoD[0]),-(manoD[1]-codoD[1]),ds[np.argmin(ds)]-(manoD[2]-codoD[2])]
    vi=[-(manoI[0]-codoI[0]),-(manoI[1]-codoI[1]),ds[np.argmin(ds)]-(manoI[2]-codoI[2])]
    
    # extrapolacion 
    vectD = [manoD[0]-codoD[0],manoD[1]-codoD[1],manoD[2]-codoD[2]]
    alfa = -manoD[2]/vectD[2]
    y=manoD[1]+alfa*vectD[1]
    x=manoD[0]+alfa*vectD[0]
    #print(x,y,'x,y DER')

    t=write_tf((x,y,0),(0,0,0,1),'pointing_right')
    b_st.sendTransform(t)
    res.x_r=x
    res.y_r=y
    res.z_r=0
    rospy.sleep(0.3)

    # extrapolacion 
    vectD = [manoI[0]-codoI[0],manoI[1]-codoI[1],manoI[2]-codoI[2]]
    alfa = -manoI[2]/vectD[2]
    y=manoD[1]+alfa*vectD[1]
    x=manoD[0]+alfa*vectD[0]
    #print(x,y,'x,y IZQ')

    t=write_tf((x,y,0),(0,0,0,1),'pointing_left')
    b_st.sendTransform(t)
    res.x_l=x
    res.y_l=y
    res.z_l=0
    rospy.sleep(0.3)

    if np.linalg.norm(vd) > np.linalg.norm(vi):
        print("Mano DERECHA levantada")
        res.x_l = -1.0
        res.y_l = -1.0
        res.z_l = -1.0
        #
    else:
        print("Mano IZQUIERDA levantada")
        res.x_r = -1.0
        res.y_r = -1.0
        res.z_r = -1.0
    return res

# FUNCIONES PARA DETECTAR TODOS LOS KEYPOINTS
#--------------------------------------
def getKeypoints(output,inWidth, inHeight,numKeys=8):
    # se obtiene primero los keypoints, no deberia dar problemas
    keypoints=[]
    for i in range (numKeys):
        probMap = output[0, i, :, :]
        probMap = cv2.resize(probMap, (inWidth, inHeight))
        mapSmooth = cv2.GaussianBlur(probMap,(3,3),0,0)
        thresh =-256 if mapSmooth.max() < 0.1 else 256
        minthresh = mapSmooth.max()*thresh/2 if thresh == 256 else mapSmooth.min()*thresh/2
        if minthresh >15:
            _,mapMask = cv2.threshold(mapSmooth*thresh,minthresh-12,255,cv2.THRESH_BINARY)
        else:
            _,mapMask = cv2.threshold(mapSmooth*thresh,minthresh-1,255,cv2.THRESH_BINARY)
        contours,_ = cv2.findContours(np.uint8(mapMask), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
        for cnt in contours:
            blobMask = np.zeros(mapMask.shape)
            blobMask = cv2.fillConvexPoly(blobMask, cnt, 1)
            maskedProbMap = mapSmooth * blobMask
            _, maxVal, _, maxLoc = cv2.minMaxLoc(maskedProbMap)
            keypoints.append(maxLoc + (probMap[maxLoc[1], maxLoc[0]],) +(i,))
    return keypoints

#----------------------
def getGroups(output,conections,inHeight,inWidth):
    
    fullMAP=np.zeros((inHeight,inWidth),np.float32)
    for con in conections:
        probMap = output[0, con, :, :]
        probMap = cv2.resize(probMap, (inWidth, inHeight))
        mapSmooth = cv2.GaussianBlur(probMap,(3,3),0,0)
        thresh =-256 if mapSmooth.max() < 0.1 else 256
        minthresh = mapSmooth.max()*thresh/2 if thresh == 256 else mapSmooth.min()*thresh/2
        if minthresh >15:
            _,mapMask = cv2.threshold(mapSmooth*thresh,minthresh-12,255,cv2.THRESH_BINARY)
        else:
            _,mapMask = cv2.threshold(mapSmooth*thresh,minthresh-1,255,cv2.THRESH_BINARY)

        fullMAP += mapMask
    contours,_ = cv2.findContours(np.uint8(fullMAP), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
    
    listOfGroups=[]
    for i in range(len(contours)):
        tmpo=np.zeros((inHeight,inWidth),np.float32)
        cv2.drawContours(tmpo, [contours[i]], -1, (255,255,0), thickness=cv2.FILLED)
        listOfGroups.append(np.transpose(np.nonzero(tmpo == 255)))
    return listOfGroups

#----------------------
# FUNCION PRINCIPAL
# version 2
def getconectionJoints(output,inHeight,inWidth,numKeyPoints = 8 ):
    conections = [57,40,43,45,48,51,53] #,27,32,34,37]   # SOLO se utilzan 7 conexiones por simplicidad
    # se obtiene primero los keypoints 
    
    keypoints = getKeypoints(output,inWidth, inHeight,numKeys = numKeyPoints)
    # cuento max personas encontradas
    conteo=Counter([i[-1] for i in keypoints])
    maxPeople = max(list(conteo.values()))
    avgPeople = round(sum(list(conteo.values()))/len(list(conteo.values())))
    if maxPeople > avgPeople :
        maxPeople = avgPeople
    
    groups = getGroups(output,conections,inHeight,inWidth)
    sk = np.zeros([len(groups),numKeyPoints,2])

    #print(maxPeople,len(groups))
    if maxPeople != len(groups):
        raise Exception("Distinto numero de KP que esqueletos detectados ")
    
    for k in keypoints:
        for i, group in enumerate(groups):
            if [True for item in group if (item == [k[1],k[0]]).all()]:     
                sk[i,k[3],0] = k[0]
                sk[i,k[3],1] = k[1]
    return sk

#----------------------
def drawSkeletons(frame,sk,plot=False):
    colors=[(41,23,255),(99,1,249),(251,10,255),(10,75,255),(41,243,186),(10,10,255),
                    (25,136,253),(40,203,253),(0,218,143),(0,218,116),(78,218,0),(253,183,31),
                    (148,241,4),(239,255,1),(253,145,31),(253,80,31),(248,8,207),(248,8,76)]
    conections = [[0,1],[1,2],[2,3],[3,4],[1,5],[5,6],[6,7]] #primero puede ser NO necesario
    
    rgbbkg = np.copy(frame)
    for s in sk:
        rgbbkg = cv2.circle(rgbbkg,(int(s[0][0]),int(s[0][1])),6,(255,255,0),-1)
        rgbbkg = cv2.circle(rgbbkg,(int(s[1][0]),int(s[1][1])),6,(255,255,0),-1)
        rgbbkg = cv2.line(rgbbkg,
                              (int(s[conections[0][0],0]),int(s[conections[0][0],1])),
                              (int(s[conections[0][1],0]),int(s[conections[0][1],1])),
                              colors[0],
                              2)
    for i in range(1,len(conections)):
        for s in sk:
            if int(s[conections[i][1],0]) != 0:
                rgbbkg = cv2.circle(rgbbkg,(int(s[conections[i][1],0]),int(s[conections[i][1],1])),6,(255,255,0),-1)
            if int(s[conections[i][0],0]) != 0 and int(s[conections[i][1],0]) != 0:
                rgbbkg = cv2.line(rgbbkg,
                              (int(s[conections[i][0],0]),int(s[conections[i][0],1])),
                              (int(s[conections[i][1],0]),int(s[conections[i][1],1])),
                              colors[i],
                              2)
    
    if plot:
        plt.imshow(rgbbkg)
    return rgbbkg

#--------------------------
# para quitar fondo, es necesario rgbd de la camara del robot
def removeBackground(points_msg,distance = 2):
    # Obtengo rgb
    points_data = ros_numpy.numpify(points_msg)
    image = points_data['rgb'].view((np.uint8, 4))[..., [2, 1, 0]]
    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)    

    # Quito todos los pixeles que esten a una distancia mayor y/o a una distancia menor
    # Para poder obtener una mascara con ceros y unos
    zs_no_nans=np.where(~np.isnan(points_data['z']),points_data['z'],10)
    img_corrected = np.where((zs_no_nans < distance + 0.3),zs_no_nans,0)
    #img_corrected = np.where((img_corrected >1.5),img_corrected,0)
    img_corrected = np.where((img_corrected == 0),img_corrected,1)

    # operacion AND entre la imagen original y la mascara para quitar fondo (background)
    #img_corrected = img_corrected.astype(np.uint8)
    masked_image = cv2.bitwise_and(rgb_image, rgb_image, mask=img_corrected.astype(np.uint8))
    return rgb_image, masked_image

#-----------------------------------------------------------------
def save_image(img,name='',dirName=''):
    rospack = rospkg.RosPack()
    file_path = rospack.get_path('images_repos')
    
    num_data = len(glob(os.path.join(file_path,"src",dirName,"*"))) if dirName else len(glob(os.path.join(file_path,"src","*")))
    
    num_data = str(num_data+1).zfill(4)

    name = "/" + name if (name and not(name.startswith("/"))) else name
    dirName = "/" + dirName if (dirName and not(dirName.startswith("/"))) else dirName

 
    if name and dirName:
        #print(file_path+"/src"+dirName+name+".jpg")
        cv2.imwrite(file_path+"/src"+dirName+name+num_data+".jpg",img)
    
    elif dirName and not(name):
        #print(file_path+"/src"+dirName+"/"+"image"+".jpg")
        cv2.imwrite(file_path+"/src"+dirName+"/"+"image"+num_data+".jpg",img)

    elif not(dirName) and name:
        #print(file_path+"/src"+name+".jpg")
        cv2.imwrite(file_path+"/src"+name+num_data+".jpg",img)
    
    else:
        #print(file_path+"/src"+"tmp"+".jpg")
        cv2.imwrite(file_path+"/src"+"image"+".jpg",img)
    